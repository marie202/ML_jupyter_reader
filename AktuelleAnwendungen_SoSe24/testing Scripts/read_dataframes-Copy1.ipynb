{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8464b787-2992-4105-9c43-141c554d85a1",
   "metadata": {},
   "source": [
    "tempfelhof import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96f603b-9ba1-471a-978e-2e912af53502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abac4a6-30b2-42a4-b5c5-3e4f8e9d234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import the used dataset:\n",
    "\n",
    "variables = [\"Jahr\",\"Monat\",\"Tag\",\"Stunde\",\"Minute\",\"WDay\",\"WoY\",\"DoY\",\n",
    "\"air_temperature\"                            ,\"air_temperature_guete\",                # C\n",
    "\"relative_humidity\"                          ,\"relative_humidity_guete\",              # %\n",
    "\"duration_of_sunshine\"                       ,\"duration_of_sunshine_guete\",           # min                       \n",
    "\"air_pressure_at_station_level\"              ,\"air_pressure_at_station_level_guete\",  # hPa              \n",
    "\"air_temperature2\"                           ,\"air_temperature2_guete\",                   # C      \n",
    "\"height_of_precipitation\"                    ,\"height_of_precipitation_guete\",            # mm       \n",
    "\"wind_speed_maximum\"                         ,\"wind_speed_maximum_guete\",                 # m/s        \n",
    "\"soil_temperature\"                           ,\"soil_temperature_guete\",                   # C        \n",
    "\"wind_speed_mean\"                            ,\"wind_speed_mean_guete\",                    # m/s       \n",
    "\"global_solar_radiation\"                     ,\"global_solar_radiation_guete\",             # W/m^2        \n",
    "\"air_temperature3\"                           ,\"air_temperature3_guete\",                   # C        \n",
    "\"air_temperature4\"                           ,\"air_temperature4_guete\",                   # C       \n",
    "\"wind_from_direction_at_wind_speed_maximum\"  ,\"wind_from_direction_at_wind_speed_maximum_guete\",  # degree \n",
    "\"wind_from_direction\"                        ,\"wind_from_direction_guete\"  ]                 # degree\n",
    "\n",
    "use_var = [\"Jahr\",\"Monat\",\"Tag\",\"Stunde\",\"Minute\",\"WDay\",\"WoY\",\"DoY\",\"air_temperature\",\n",
    "                    \"relative_humidity\",\"duration_of_sunshine\",\"air_pressure_at_station_level\",\n",
    "                    \"height_of_precipitation\",\"wind_speed_maximum\" ,\"soil_temperature\" ,\"wind_speed_mean\" ,\n",
    "                    \"global_solar_radiation\", \"wind_from_direction_at_wind_speed_maximum\",\"wind_from_direction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6962f36d-eed7-4322-93e5-c277544750af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '01_Tempelhof-1_2006_Daten.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m01_Tempelhof-1_2006_Daten.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index() \n\u001b[1;32m      4\u001b[0m data2use \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      5\u001b[0m data2use \u001b[38;5;241m=\u001b[39m data2use\u001b[38;5;241m.\u001b[39mloc[:, use_var] \n",
      "File \u001b[0;32m/opt/anaconda3/envs/lehre/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lehre/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lehre/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lehre/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lehre/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '01_Tempelhof-1_2006_Daten.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('01_Tempelhof-1_2006_Daten.csv', skiprows = 45, \n",
    "                   sep = \"\\t\", index_col=False,\n",
    "                   names = variables, low_memory=False).reset_index() \n",
    "data2use = data.dropna()\n",
    "data2use = data2use.loc[:, use_var] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8b1aa-0b17-4016-b390-8ccffba2e902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e17d7-03d9-4d7b-9b05-db49c8e7a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tempelhof weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723dcc45-a579-4830-b06a-5c1628bcac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bitte Pfad anpassen: \n",
    "df = pd.read_csv('climate_data_Berlin_Tempelhof.txt',  header = [0], sep=';', skipinitialspace = True)\n",
    "df = df.drop(['QN_4', 'RSKF',  'SHK_TAG',  'VPM', 'eor'], axis=1)\n",
    "df = df[['KlimadateSTATIONS_ID', 'MESS_DATUM', 'QN_3', 'TGK', \"TNK\", \"TMK\", \"TXK\", \"UPM\", \"FM\", \"FX\", \"SDK\", \"NM\", \"RSK\", \"PM\"]]\n",
    "df.columns = ['STAT','JJJJMMDD', 'QN', 'TG', 'TN', 'TM', 'TX', 'RFM', 'FM', 'FX', 'SO', 'NM', 'RR', 'PM']\n",
    "\n",
    "\n",
    "## show the first 4 rows of data of the table:\n",
    "#df.head(4)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ef0e6-c555-43f4-a1f5-615e93cf5b39",
   "metadata": {},
   "source": [
    "**Dateninfos**: Beschreibungen des Deutschen Wetterdienstes:\n",
    "    \n",
    "**QN** Qualitätsniveau der Daten <br>\n",
    "**TG** Minimum der Temperatur in 5 cm über dem Erdboden Grad C TIII des Vortages \n",
    "    bis TI 23:51 Vortag - 23:50 UTC <br>\n",
    "**TN** Minimum der Temperatur in 2 m über dem Erdboden Grad C TIII des Vortages \n",
    "   bis TIII 23:51 Vortag - 23:50 UTC <br>\n",
    "**TM** Mittel der Temperatur in 2 m über dem Erdboden Grad C (TI+TII+2*TIII) / 4 \n",
    "    Mittel aus 24 Terminwerten <br>\n",
    "**TX** Maximum der Temperatur in 2 m über dem Erdboden Grad C TIII des Vortages \n",
    "    bis TIII 23:51 Vortag - 23:50 UTC <br>\n",
    "**RFM** Mittel der relativen Feuchte % (TI+TII+TIII)/3 Mittel aus 24 Terminwerten <br>\n",
    "**FM** Mittel der Windstärke Bft (TI+TII+TIII)/3 Mittel aus 24 Terminwerten <br>\n",
    "**FX** Maximum der Windgeschwindigkeit (Spitzenböe) m/sec 00 – 23:59 UTC 23:51 Vortag - 23:50 UTC <br>\n",
    "**SO** Summe der Sonnenscheindauer Stunden 00 – 23:59 UTC 23:51 Vortag - 23:50 UTC <br>\n",
    "**NM** Mittel des Bedeckungsgrades Achtel (TI+TII+TIII)/3 Mittel aus 24 Terminwerten <br>\n",
    "**RR** Niederschlagshöhe mm TI bis TI des Folgetages 05:51 - 05:50 UTC des Folgetages <br>\n",
    "**PM** Mittel des Luftdruckes in Stationshöhe hpa (TI+TII+TIII)/3 Mittel aus 24 Terminwerten \n",
    "\n",
    "Im Allgemeinen gilt für die Berechnung der Tagesmittelwerte: \n",
    "\n",
    "Ab dem 01.04.2001 wurde der Standard wie folgt geändert: \n",
    "\n",
    "Berechnung der Tagesmittel aus 24 Stundenwerten \n",
    "Wenn mehr als 3 Stundenwerte fehlen -> Berechnung aus den 4 Hauptterminen (00, 06, 12, 18 UTC). Bezugszeit für einen Tag i.d.R. 23:51 UTC des Vortages bis 23:50 UTC \n",
    "nur der Niederschlag des Vortages wird morgens um 05:50 UTC gemessen \n",
    "\n",
    "Hierbei werden die Beobachtungstermine auf die global genutzte Zeit in Greenwich (GMT oder UTC) bezogen. Die Beobachtungszeit ist jeweils 10 Minuten vor dem Bezugstermin \n",
    "(daher die krummen Zeitangaben). Diese Umstellung war erforderlich, nachdem das Stationsnetz  weitgehend automatisiert wurde. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164c07e-eab1-48bf-b43c-93221a34579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"TG\"], stat='density', kde=True, kde_kws={\"cut\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e885a-8979-45ff-a137-99465d5d2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sm.qqplot(df[\"TG\"], line=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03307077-a5e2-440d-a098-971077959da3",
   "metadata": {},
   "source": [
    "for i, col in enumerate(df.columns[3:14]):\n",
    "    plt.figure(i)\n",
    "    sns.histplot(df[col], stat='density', kde=True, kde_kws={\"cut\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35088b6a-d41b-4567-a49c-5184ac6ce35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4281447-2105-44e2-957b-f98e7691fe96",
   "metadata": {},
   "source": [
    "### NetCDF files\n",
    "\n",
    "[towardsdatescience](https://towardsdatascience.com/read-netcdf-data-with-python-901f7ff61648)\n",
    "\n",
    "[NOAA](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/python_netcdf4.php)\n",
    "\n",
    "Network common data form (NetCDF) is commonly used to store multidimensional geographic data. Some examples of these data are temperature, precipitation, and wind speed. Variables stored in NetCDF are often measured multiple times per day over large (continental) areas. With multiple measurements per day, data values accumulate quickly and become unwieldy to work with. When each value is also assigned to a geographic location, data management is further complicated. NetCDF provides a solution for these challenges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bdccc-be30-4f81-9fce-c93b6b294d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import netcdf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ec88d-0461-470d-aa80-528350329f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import libraries\n",
    "import netCDF4 # import libraries\n",
    "fp='data/HYDRAS_Data/tas_hyras_5_1981_v5-0_de.nc' # your file name with the eventual path\n",
    "nc = netCDF4.Dataset(fp) # reading the nc file and creating Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd3c31-48a9-47f5-97ac-df4b868d6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75866b-5ef1-43cf-b032-b6599fc919b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = nc.variables['number_of_stations']  # access a variable in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b45b5-8a87-4434-910d-3e0bb4ebd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.variables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b3529-d3f4-4bb2-ba30-56d95574d3db",
   "metadata": {},
   "source": [
    "Read in temp (tas) data\n",
    "> Metadata indicates data are 2-dimensional so use [:,:] to extract all data in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b558f5b-aa7e-4033-947c-c0382985ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coord vars associated with this variable\n",
    "#for dname in nc.dimensions:\n",
    "#  print(nc.variables[dname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48614fb2-f925-421e-8ef7-a28f9608263f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a1a98-fa60-476f-a222-bd096d25aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = nc.variables['tas'][:,:]\n",
    "\n",
    "# Print max and min of FRP array to check data range\n",
    "print('The maximum Temp value is', np.max(tas), nc.variables['tas'].units)\n",
    "print('The minimum Temp value is', np.min(tas), nc.variables['tas'].units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cea2da-0e4a-4728-bad6-260ecc5ec70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metadata for the \"x\" variable\n",
    "print(nc.variables['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3375016-d924-4262-b476-4ccd7c776114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GOES ABI fixed grid projection x-coordinate data\n",
    "# Metadata indicates data are 1-dimensional so use [:] to extract all data in 1 dimension\n",
    "x_coordinate = nc.variables['x'][:]\n",
    "\n",
    "# Metadata indicates data are stored as integers (\"float32\") in the .nc file\n",
    "# netCDF4 library automatically applies \"scale_factor\" and \"add_offset\" to covnvert stored integers (\"int16\") to floats\n",
    "# Check data type of first value in array to confirm\n",
    "print('Array data values are type', type(x_coordinate[0]))\n",
    "\n",
    "# Print x-coordinate array\n",
    "print(x_coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61596f7c-14b4-4be7-b580-112a12e6cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = nc.variables['tas'][:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce06f1-1756-4336-ac1c-c8859e2bc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = nc.variables['tas']\n",
    "print(tas)\n",
    "print(\"================\")\n",
    "print(tas.missing_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ed117-c113-4742-8705-4b2d49091df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip the data in latitude so North Hemisphere is up on the plot\n",
    "temp = tas[0,::-1,:]\n",
    "print('shape=%s, type=%s, missing_value=%s' % \\\n",
    " (temp.shape, type(temp), tas.missing_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98681beb-3bcc-42fa-a65a-b9d9314a5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip the data in latitude so North Hemisphere is up on the plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "cs = plt.contourf(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47486c57-b65e-45e6-a27f-926431c3a977",
   "metadata": {},
   "source": [
    "### Dealing with Dates and Times\n",
    "The time variables are usually measured relative to a fixed date using a certain calendar. The specified units are like “hours since YY:MM:DD hh:mm:ss”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943c199-d30c-4b85-9b39-aff797e60f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import num2date, date2num, date2index\n",
    "timedim = nc.dimensions[\"time\"] # time dim name\n",
    "print('name of time dimension = %s' % timedim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a109f11-5434-4ed3-9790-45e6b4102751",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = nc.variables[\"time\"] # time coord var\n",
    "print('units = %s, values = %s' % (times.units, times[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0312526-d671-4baf-b389-a35a3d7a5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = num2date(times[:], times.units)\n",
    "print([date.strftime('%Y-%m-%d %H:%M:%S') for date in dates[:10]]) # print only first ten...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5e3fb-280d-4a57-b461-249b4263cc7a",
   "metadata": {},
   "source": [
    "We can also get the index associated with the specified date and forecast the data for that date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f7cdc-ddce-4aab-ade6-77e4521b2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime as dt\n",
    "date = dt.datetime.now() + dt.timedelta(days=3)\n",
    "print(date)\n",
    "ntime = date2index(date,times,select='nearest')\n",
    "print('index = %s, date = %s' % (ntime, dates[ntime]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bd1ba-27f9-4315-bd03-45dfe6fac4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccd781-cb2e-4605-a2b7-3b72beed5c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace 'your_file.nc' with the path to your NetCDF file\n",
    "file_path = 'data/HYDRAS_Data/tas_hyras_5_1981_v5-0_de.nc'\n",
    "\n",
    "# Read the NetCDF file into an xarray Dataset\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "ts = ds['tas']\n",
    "\n",
    "# Access latitude and longitude variables\n",
    "lats = ds['lat'].values\n",
    "lons = ds['lon'].values\n",
    "\n",
    "# Display the dataset information\n",
    "print(ds)\n",
    "\n",
    "# Close the dataset when done\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7f4df-2ce4-41cd-aae9-d08c03af8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbf57d-bd3c-4bbf-a117-692929040219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a10d47-ee28-4ccf-83d5-73a9cac3020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ein gebiet durch lat lon heraus filtern\n",
    "#### z:B. Berlin 52°30'N\t13°25'E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980c4ec-76f6-401e-bdc1-96af3eb1155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lats), len(lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fa566-04fa-4d59-b555-c048a3d7aeb0",
   "metadata": {},
   "source": [
    "## 1. batch download .nc files from online ressource\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed3187-5783-44ec-ba73-337dd2e6258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import wget \n",
    "\n",
    "\n",
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "802f0655-11ba-49e9-8e66-40b4e3496187",
   "metadata": {},
   "source": [
    "## sample name file: tas_hyras_5_1951_v5-0_de.nc\n",
    "\n",
    "year = 1981\n",
    "file_start_name = \"tas_hyras_5_\"\n",
    "file_end_name = \"_v5-0_de\"\n",
    "file_ext_name = \".nc\"\n",
    "\n",
    "file_name = f\"{file_start_name}{year}{file_end_name}{file_ext_name}\"\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cbd97-1c64-4357-98ab-253add9b1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set current WD\n",
    "os.chdir(pwd)\n",
    "\n",
    "## specify years to download\n",
    "start_year = 1981 #yyyy\n",
    "end_year = 1983 #yyy\n",
    "## specify wd\n",
    "path = \"data/\"\n",
    "download_dir = os.path.join(path, \"HYDRAS_Data\")\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "\n",
    "## sample link to hydras data: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/\n",
    "base_site = \"https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/\"\n",
    "## sample name file: tas_hyras_5_1951_v5-0_de.nc\n",
    "\n",
    "##########################\n",
    "\n",
    "for year in range (start_year, end_year+1,1):\n",
    "    file_start_name = \"tas_hyras_5_\"\n",
    "    file_end_name = \"_v5-0_de\"\n",
    "    file_ext_name = \".nc\"\n",
    "    file_name = f\"{file_start_name}{year}{file_end_name}{file_ext_name}\"\n",
    "    url = f\"{base_site}/{file_name}\"\n",
    "    print(f\"Downloading .....{file_name}\")\n",
    "    wget.download(url, download_dir)\n",
    "    print(f\"{file_name}.....Downloaded successfully\")\n",
    "print(\"All Data successfully downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8438e-3267-4362-99eb-f8916ba261e7",
   "metadata": {},
   "source": [
    "### 2. Extraction of required variables from the netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96e1c4-5a09-400d-a19a-6917fe6bd089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab145f5-0d1f-4245-ade9-b5954030813e",
   "metadata": {},
   "source": [
    "#### Understading the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc100c-59ac-40f8-b648-135b7d891084",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp='data/HYDRAS_Data/tas_hyras_5_1981_v5-0_de.nc' # your file name with the eventual path\n",
    "nc = netCDF4.Dataset(fp) # reading the nc file and creating Dataset\n",
    "nc.variables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d56749-902e-4a57-80da-115d7ad636fc",
   "metadata": {},
   "source": [
    " > check the timing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c43e8-854f-432c-82ab-a3f81ccc7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.variables['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859588bc-aac2-45d9-8e85-d8979b19ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.variables['time'].units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94dedad-1aa4-4266-bdf7-587318760be2",
   "metadata": {},
   "source": [
    "Ok, we are dealing with hourly data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bceddc8-2a9e-4a04-9315-99faa941a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extractt varables as needed to make TS\n",
    "tas = nc.variables['tas'][:] # mean temp\n",
    "times = nc.variables['time'][:]\n",
    "lat = nc.variables['x'][:] # lat\n",
    "lon = nc.variables['lon'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f00c6-00ee-4c4b-bad2-c2574ae5458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nc.variables['x'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b27be8-88e7-44ff-8ac2-235138cf941f",
   "metadata": {},
   "source": [
    "## Subset data to AOI\n",
    " > avoid memory error with large datastets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9fa1b-db51-4376-b768-3cc5b8c23038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "import numpy\n",
    "import time\n",
    "from datetime import date\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b1355-9bb8-4658-bebe-54bd371f7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a7c92-8585-41d1-9ef6-2b8a325b0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "\n",
    "#output_dir = os.path.join(path, \"HYDRAS-data_clip\") # \n",
    "output_dir = \"/Users/marie-christineckert/Nextcloud/TU/Lehre/Advanced/testing Scripts/data/HYDRAS-data_clip/\"#\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "input_dir = os.path.join(path, \"HYDRAS_Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15570ed-fd47-4a39-902d-56f31c6c6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the boundary of the suubset basically the lowerleft nad upper right corner\n",
    "# This area is Berlin, germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa30163-a572-4c5e-becb-81e6cd42d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90651915-605b-472e-a502-27e8fb7c8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "latbound = [0,40]\n",
    "lonbound = [60,100]\n",
    "\n",
    "os.chdir(input_dir)\n",
    "for filename in os.listdir():\n",
    "    if filename.endswith('.nc'):\n",
    "        file_name, file_ext = os.path.splitext(filename)\n",
    "        new_filename = f\"{file_name}_clip{file_ext}\"\n",
    "        new_ncfile = f\"{output_dir}/{new_filename}\"\n",
    "        print(f\"Now reading {filename}\")\n",
    "        data = netCDF4.Dataset(filename, 'r')\n",
    "        # reading the data from the main netcdf file to be subset\n",
    "        print(\"reading the data from the main netcdf file to be subset!\")\n",
    "        ###################### \n",
    "        print(\"reading all values of lat lon and time\")\n",
    "        tims = data.variables['time'][:]\n",
    "        lats = data.variables['lat'][:]\n",
    "        lons= data.variables['lon'][:]\n",
    "        print(\"Reading completed!\")\n",
    "\n",
    "        # Calculate min values of diff between lats lons of main data and customize boundary to make the bounding box\n",
    "        lat_lb = np.argmin(abs(lats-latbound[0]))\n",
    "        lat_ub = np.argmin(abs(lats-latbound[1]))\n",
    "        lon_lb = np.argmin(abs(lons-lonbound[0]))\n",
    "        lon_ub = np.argmin(abs(lons-lonbound[1]))\n",
    "\n",
    "        # reading the varibales values only in between the range of the bounding box\n",
    "        print(\"reding the varibales values only in between the range of the bounding box\")\n",
    "        lat_sub = data.variables['lat'][lat_lb:lat_ub]\n",
    "        lon_sub = data.variables['lon'][lon_lb:lon_ub]\n",
    "        print(\"Reading completed!\")  \n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"creating new netcdf file!\")\n",
    "        my_file = netCDF4.Dataset(new_ncfile, 'w', format  = \"NETCDF4\")\n",
    "        my_file.Conventions = data.Conventions\n",
    "        my_file.title = data.title\n",
    "        #my_file.history = data.history\n",
    "        #my_file.version = data.version\n",
    "        #my_file.date_created  = data.date_created\n",
    "        #my_file.creator_name\n",
    "        #my_file.creator_email \n",
    "        #my_file.institution\n",
    "        #my_file.documentation\n",
    "        my_file.date_subsetted = str(date.today())\n",
    "       # my_file.reference = data.reference\n",
    "        #my_file.comments = data.comments\n",
    "        #my_file.ftp_url = data.ftp_url\n",
    "        print(\"new netcdf file created in output directory!\")\n",
    "        print(\"creating dims of new variables!\")\n",
    "        ldim = abs(lat_lb - lat_ub)\n",
    "        lndim = abs(lon_ub - lon_lb)\n",
    "        tdim = len(tims)\n",
    "        longitude = my_file.createDimension(\"longitude\", lndim)\n",
    "        latitude = my_file.createDimension(\"latitude\", ldim)\n",
    "        time = my_file.createDimension(\"time\", tdim)\n",
    "        print(\"DONE!\")\n",
    "        print(\"creating variables of new netcdf file!\")\n",
    "        \n",
    "        lat = my_file.createVariable(\"latitude\", np.float32, (\"latitude\",))\n",
    "        lat.units = data.variables['lat'].units\n",
    "        lat.standard_name = data.variables['lat'].standard_name\n",
    "        #lat.axis = data.variables['lat'].axis\n",
    "        \n",
    "        lon = my_file.createVariable(\"longitude\", np.float32, (\"longitude\",))\n",
    "        lon.units = data.variables['lon'].units\n",
    "        lon.standard_name = data.variables['lon'].standard_name\n",
    "#        lon.axis = data.variables['lon'].axis\n",
    "\n",
    "        tp = my_file.createVariable(\"tas\", np.float32, (\"time\", \"latitude\", \"longitude\",), fill_value = -9999.0)\n",
    "        tp.units = data.variables['tas'].units\n",
    "        tp.standard_name = data.variables['tas'].standard_name\n",
    "        tp.long_name = data.variables['tas'].long_name\n",
    "        #tp.time_step = data.variables['tas'].time_step\n",
    "        \n",
    "        tp.geospatial_lat_min = 0.0\n",
    "        tp.geospatial_lat_max = 40.0\n",
    "        tp.geospatial_lon_min = 50.0\n",
    "        tp.geospatial_lon_min = 110.0\n",
    "\n",
    "        times = my_file.createVariable(\"time\", np.float32, (\"time\",))\n",
    "        times.units = data.variables['time'].units\n",
    "        times.standard_name = data.variables['time'].standard_name\n",
    "        times.calendar = data.variables['time'].calendar\n",
    "      #  times.axis = data.variables['time'].axis\n",
    "\n",
    "        print(\"successfully created the varaibles of the new netcdf file!\")\n",
    "\n",
    "        print(\"Started writing the variables sto the new netcdf file!\")\n",
    "        lat[:] = lat_sub\n",
    "        lon[:] = lon_sub\n",
    "        times[:] = tims\n",
    "        tp[:,:,:] = data.variables[\"tas\"][:, lat_lb:lat_ub, lon_lb:lon_ub] \n",
    "        print(\"closing files and clearing memory!\")\n",
    "        data.close()\n",
    "        my_file.close()\n",
    "        del data\n",
    "        del my_file  \n",
    "        del tp\n",
    "        gc.collect()\n",
    "        print(\"Memory cleared!\")\n",
    "print(\"##################\")\n",
    "print(\"! Subseting of netcdf file is successful !\")\n",
    "print(\"##################\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c6417-ebf1-41a4-a964-bd9e1cbc2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bf13c-8659-410e-8991-381e45891544",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79ac9f-ad28-4139-b40a-397d198557c9",
   "metadata": {},
   "source": [
    "## lets plot the variables again to see the spatial extent of the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7534f52-6f5b-4b62-bb2e-2831f3608d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(pwd)\n",
    "\n",
    "fp='data/HYDRAS-data_clip//tas_hyras_5_1981_v5-0_de_clip.nc' # your file name with the eventual path\n",
    "\n",
    "data = netCDF4.Dataset(fp, \"r\") # reading the nc file and creating Dataset\n",
    "data.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78695e-6c9c-4496-bdec-65c082b66a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract variables to make time series\n",
    "\n",
    "tas = data.variables['tas'][:] \n",
    "unittas = data.variables['tas'].units\n",
    "unitt = data.variables['time'].units\n",
    "times  = data.variables['time'][:] \n",
    "\n",
    "## storing lat and lon \n",
    "lat  = data.variables['lat'][:] \n",
    "lon  = data.variables['lon'][:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bced1-c14c-4f84-9edf-3c17f153f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lat,lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173418e-0d5a-40c1-b199-d1011233438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "#  pip install basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d92f8-0dcd-43b2-8a27-dc80e509ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,10))\n",
    "map = Basemap(projection = \"mill\", \n",
    "              llcrnrlat = 40,\n",
    "              urcrnrlat = 60,\n",
    "              llcrnrlon = -50,\n",
    "              urcrnrlon = 53,\n",
    "              resolution = \"c\")\n",
    "map.drawcoastlines() \n",
    "map.drawcountries()\n",
    "map.drawlsmask(land_color = \"Linen\", ocean_color = \"#CCFFFF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb697cc-b8a7-476b-a358-5ce24201ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87cd36-6eaa-4905-b966-cc637b712338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243774f-2fee-4179-897d-a0f22886afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons, lats = np.meshgrid(lon,lat)\n",
    "#x,y = map(lons, lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a086ce-5cec-40e7-9b76-b70ebe150b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d21e6-fc84-49a8-8596-d6f1be19c222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc86fd-a972-46ef-9731-1fae0fa590d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543096c-2c38-41b2-beba-a4648ed408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clip by raster\n",
    "\n",
    "import xarray as xr\n",
    "fp='data/HYDRAS_Data/tas_hyras_5_1981_v5-0_de.nc'\n",
    "data = xr.open_dataset(fp)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea03c0-063a-4010-868c-9bcfc26244b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a28eb00d-65f9-4667-ab2d-a285966feb22",
   "metadata": {},
   "source": [
    "> Gute Doku für xarray : https://docs.xarray.dev/en/v0.7.2/data-structures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a603b0-9128-4c56-9662-c3d091605f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb22eb0-8d36-4775-8d39-d21fca58ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin = gpd.read_file(\n",
    "    \"http://userpage.fu-berlin.de/soga/soga-py/300/30800_spatial_point_patterns/berlin_district.zip\", crs=\"epsg:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c894f8-dcb6-4b9e-ac11-896d6a7159ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin = berlin.to_crs(\"epsg:4326\") \n",
    "\n",
    "berlin = berlin.to_crs('epsg:3034')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09498ff6-10c3-44f7-bab4-51738eb5c5bf",
   "metadata": {},
   "source": [
    "Bei der Lat/ Lon angabe gilt folgendes : \n",
    "\n",
    "https://gis.stackexchange.com/questions/378861/strange-netcdf-file-with-lat-long-as-2d-dimensions\n",
    "\n",
    "2D Longitude and Latitude coordinates occur when the grid lines do not follow lines of constant latitude and longitude. So any projected grid, rotated grid, or curvilinear grid, even if in geographic coordinates, will have 2D longitude and latitude coordinate variables.\n",
    "\n",
    "Panoply should be able to work with these, as well as Python packages such as Xarray. But you won't be able to use some of the nice tools to slice along 1D coordinates.\n",
    "\n",
    "To extract data from a grid location nearest to a specified lon,lat point, for example, you need to write your own code, like in: https://stackoverflow.com/questions/58758480/xarray-select-nearest-lat-lon-with-multi-dimension-coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46f8da-3094-4f9f-8948-09fe08d80a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db18d87-ecf9-4ee0-9a14-47aba8b2b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data#.variables#[\"lon\"][0].values\n",
    "#data.variables[\"x\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7ad72-0659-4a01-ace7-64c7fd076f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variables['lat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec2cee-248a-4f6a-8598-5098a6644ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.crs_HYRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ef2d4-2ecb-44cd-a372-a0bcb56ebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minx, miny, maxx, maxy = berlin.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5089477-669b-4e20-8640-b90221f8dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "miny, maxy, minx, maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6cbf8-cf65-4d26-9e8e-9fb3ffd838da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop  = data.sel(x=slice(minx, maxx), y=slice(miny, maxy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a11c3-a046-4dc6-b3cc-b7cbbf840631",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42737c-f64c-4c00-8097-788c00c60152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "data_crop.tas.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4aa86-aa73-4750-943b-d26b77da75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data_crop['tas'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd712488-239b-4dda-b899-395530e5a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop.variables['x'][::] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e7ca5-f102-4bba-9bf0-37ac05a09828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop.isel(x=1, y= 1)# for one specific location\n",
    "plt.plot(data_crop.isel(x=1, y= 1)['time'].values, \n",
    "         data_crop.isel(x=1, y= 1)['tas'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c90218-45ee-432f-8bfb-422ea35afb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop2d = data_crop.isel(time=200)\n",
    "\n",
    "data_crop2d.tas.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2d479da-aed5-47b2-81be-da126ff0e45f",
   "metadata": {},
   "source": [
    "lats = data_crop.variables['x'][::]  # extract/copy the data\n",
    "lons = data_crop.variables['y'][::]\n",
    "PM25 = data_crop.variables['tas'][::]\n",
    "\n",
    "fig, axs = plt.subplots()#figsize=(15, 10), nrows=2,ncols=1,gridspec_kw={'height_ratios': [20,1.5]},constrained_layout=True)\n",
    "\n",
    "\n",
    "pcm=axs.pcolormesh(lons,lats,PM25,cmap='viridis')\n",
    "\n",
    "\n",
    "#cbar=fig.colorbar(pcm,cax=axs[1], extend='both', orientation='horizontal')\n",
    "#cbar.set_label('PM 2.5 [$\\mu$g m$^{-3}]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384dd7a-7a14-4d41-9119-5f227fcd3699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf166d-a3fc-4d24-91d6-8022cb42a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
